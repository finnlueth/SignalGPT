{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as model_config\n",
    "from src.utils import get_project_root_path\n",
    "import src.data\n",
    "import src.metrics\n",
    "import src.plot\n",
    "\n",
    "from src.model import (\n",
    "    ProstT5EncoderModelForSequenceClassification,\n",
    "    ProstT5EncoderModelTokenClassificationCRF,\n",
    "    ProstT5EncoderModelTokenClassificationLinear,\n",
    "    ProtT5EncoderModelForSequenceClassification,\n",
    "    ProtT5EncoderModelTokenClassificationCRF,\n",
    "    ProtT5EncoderModelTokenClassificationLinear,\n",
    ")\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:\t Rostlab/prot_t5_xl_uniref50\n",
      "MPS Availible:\t False\n",
      "Path:\t\t /home/ec2-user/src/SignalGPT\n",
      "Using device:\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "ROOT = get_project_root_path()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "EXPERT = model_config.selected_expert\n",
    "MODEL_VERRSION = model_config.model_version\n",
    "\n",
    "adapter_location = f'/models/moe_v{MODEL_VERRSION}_CRF_expert_{EXPERT}'\n",
    "\n",
    "SEED = model_config.seed\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Base Model:\\t\", model_config.base_model_name)\n",
    "print(\"MPS Availible:\\t\", torch.backends.mps.is_available())\n",
    "print(\"Path:\\t\\t\", ROOT)\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_config.base_model_name,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 13288\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "FASTA_FILENAME = model_config.dataset_name\n",
    "# FASTA_FILENAME = '6_SignalP_6.0_Training_set_testing.fasta'\n",
    "annotations_name = ['Label'] #+ ['Type'] # Choose Type or Label\n",
    "\n",
    "df_data = src.data.process(src.data.parse_file(ROOT + '/data/raw/' + FASTA_FILENAME))\n",
    "\n",
    "dataset_signalp_type_splits = {}\n",
    "\n",
    "for sequence_type in model_config.select_encoding_type.keys():\n",
    "    dataset_signalp = src.data.create_datasets(\n",
    "        splits=model_config.splits,\n",
    "        tokenizer=t5_tokenizer,\n",
    "        data=df_data,\n",
    "        annotations_name=annotations_name,\n",
    "        dataset_size=model_config.dataset_size,\n",
    "        sequence_type=sequence_type\n",
    "        )\n",
    "    dataset_signalp_type_splits.update({sequence_type: dataset_signalp})\n",
    "\n",
    "del df_data\n",
    "\n",
    "dataset_signalp = dataset_signalp_type_splits[EXPERT]\n",
    "print(EXPERT)\n",
    "# print(dataset_signalp_type_splits)\n",
    "print(dataset_signalp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/src/SignalGPT/.venv/lib64/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of ProstT5EncoderModelTokenClassificationCRF were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['crf._constraint_mask', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'custom_classifier.bias', 'custom_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,946,510 || all params: 1,212,088,478 || trainable%: 0.32559586792805073\n"
     ]
    }
   ],
   "source": [
    "t5_base_model = ProstT5EncoderModelTokenClassificationCRF.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_config.base_model_name,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=False,\n",
    "    custom_num_labels=len(model_config.select_decoding_type[EXPERT]),\n",
    "    custom_dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "tmp_lin = nn.Linear(\n",
    "    in_features=t5_base_model.config.hidden_size,\n",
    "    out_features=t5_base_model.custom_num_labels\n",
    ")\n",
    "t5_base_model.custom_classifier.weight = tmp_lin.weight\n",
    "t5_base_model.custom_classifier.bias = tmp_lin.bias\n",
    "\n",
    "t5_base_model.crf._constraint_mask = torch.nn.Parameter(t5_base_model.crf.tensor_constraint_mask, requires_grad=False)\n",
    "\n",
    "t5_base_model.crf.reset_parameters()\n",
    "modules_to_save = ['custom_classifier', 'crf']\n",
    "\n",
    "modules_to_save = ['custom_classifier']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q', 'k', 'v', 'o'],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=modules_to_save,\n",
    ")\n",
    "\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=t5_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ROOT+'/models/checkpoints',\n",
    "    learning_rate=model_config.lr,\n",
    "    per_device_train_batch_size=model_config.batch_size,\n",
    "    per_device_eval_batch_size=model_config.batch_size*2,\n",
    "    num_train_epochs=model_config.num_epochs,\n",
    "    logging_steps=model_config.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=model_config.eval_steps,\n",
    "    # weight_decay=0.01,\n",
    "    # gradient_accumulation_steps=accum,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=model_config.save_steps,\n",
    "    # save_total_limit=5,\n",
    "    # load_best_model_at_end=True,\n",
    "    # fp16=True,\n",
    "    # deepspeed=deepspeed_config,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    "    seed=42,\n",
    "    # debug=\"underflow_overflow\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_signalp['train'],\n",
    "    eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=src.metrics.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24/219 00:30 < 04:22, 0.74 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n",
      "tensor.shape 2 torch.Size([32, 71])\n",
      "tensor.shape 1 torch.Size([32, 69, 7])\n",
      "tensor.shape 2 torch.Size([32, 69, 7])\n",
      "encoder_outputs...\n",
      "truncate\n",
      "crf\n",
      "log\n",
      "done\n",
      "tensor.shape 1 torch.Size([32, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial_validation\u001b[38;5;241m=\u001b[39m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m added_initial_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(initial_validation)\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/transformers/trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3094\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3095\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3105\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/transformers/trainer.py:3296\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3294\u001b[0m     losses_host \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;28;01mif\u001b[39;00m losses_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(losses_host, losses, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3296\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_decode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3298\u001b[0m     inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(inputs_decode, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2294\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:412\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:681\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    678\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:135\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    127\u001b[0m         {\n\u001b[1;32m    128\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         }\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m~/src/SignalGPT/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:660\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor.shape 1\u001b[39m\u001b[38;5;124m'\u001b[39m, tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 660\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor.shape 2\u001b[39m\u001b[38;5;124m'\u001b[39m, tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    662\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "initial_validation=trainer.evaluate()\n",
    "added_initial_validation = False\n",
    "print(initial_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_validation=trainer.evaluate()\n",
    "print(final_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'training_log' not in locals():\n",
    "    training_log = pd.DataFrame(trainer.state.log_history)\n",
    "else:\n",
    "    training_log = pd.concat([training_log, pd.DataFrame(trainer.state.log_history)], ignore_index=True)\n",
    "if not added_initial_validation:\n",
    "    added_initial_validation = True\n",
    "    training_log = pd.concat([pd.DataFrame([initial_validation]), training_log], ignore_index=True)\n",
    "display(training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'eval_confusion_matrix' in training_log.columns:\n",
    "    training_log['eval_confusion_matrix'] = training_log['eval_confusion_matrix'].apply(lambda x: x.tolist() if type(x)==np.ndarray else None)\n",
    "t5_lora_model.save_pretrained(ROOT + adapter_location)\n",
    "training_log.to_csv(ROOT + adapter_location + '/training_log.csv', index=False)\n",
    "training_log.to_parquet(ROOT + adapter_location + '/training_log.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_log = pd.read_parquet(ROOT + f'/models/moe_v{MODEL_VERRSION}_linear_expert_{EXPERT}/training_log.parquet')\n",
    "# adapter_location = f'/models/moe_v{MODEL_VERRSION}_expert_{EXPERT}'\n",
    "training_log = pd.read_parquet(ROOT + adapter_location + '/training_log.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.plot.confusion_matrix_plot(\n",
    "    np.array(training_log['eval_confusion_matrix'][training_log['eval_confusion_matrix'].notnull()].iloc[-1].tolist()),\n",
    "    model_config.select_decoding_type[EXPERT]\n",
    "    )\n",
    "plt.savefig(ROOT + adapter_location + '/fig_cm.jpg', dpi=400)\n",
    "\n",
    "src.plot.loss_plot(training_log)\n",
    "plt.savefig(ROOT + adapter_location + '/fig_loss.jpg', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
